

\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{xr}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{bm}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{bbold}
\usepackage[T1]{fontenc}
\usepackage{subfigure}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{mydef}{Definition}
%SetFonts

%SetFonts


\title{}
\author{}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle 
\section{General Notations}

\begin{itemize}

	\item {$N$: Number of users}
	
	\item {$n$: Number of items,    $
	n = \left \{
	\begin{aligned}
	&2m-1, && \text{if n is odd} \\
	&2m, && \text{otherwise}
	\end{aligned} \right.
	$}
	
	\item {$\{A_1, ..., A_n\}$: a set of $n$ items}
	\item{$\mathcal{P}_n$: the space of permutation of $n$ integers}
	\item {$\bm{R}^1,...,\bm{R}^N$: full rankings given by the users}
	
	
	\item {$\bm{R}^j \in \mathcal{P}_n$= \{$R^j_1,...,R^j_n$\} $\sim$ Mallows($\bm{\rho}^0, \alpha^0$), defined as 
	$P(\bm{R}^j|\alpha^0, \bm{\rho}^0) = \frac{\text{exp}\{-\frac{\alpha^0}{n}d(\bm{R}^j, \bm{\rho}^0)\}}{\sum\limits_{\bm{r}\in \mathcal{P}_n}\text{exp}\{-\frac{\alpha^0}{n}d(\bm{r}, \bm{\rho}^0)\}}$}
	
	
	\item {$P(\bm{\rho}|\bm{R}^1,...,\bm{R}^N,\alpha^o)$ = $\frac{P(\bm{R}_1, ...,\bm{R}_N|\alpha^0, \bm{\rho})\cdot P(\bm{\rho})}{\sum\limits_{\bm{\rho}\in\mathcal{P}_n}P(\bm{R}_1, ...,\bm{R}_N|\alpha^0, \bm{\rho})\cdot P(\bm{\rho})}$: Mallows posterior}
	
	
	\item{\{$i_1, ..., i_n$\}: a ranking of $n$ items that determines the sequence following which the items are to be sampled. i.e. $i_j = k$ indicates that item $j$ is the k-th item is to be sampled}
	
	\item{ \{$o_1, ..., o_n$\}: an ordering of $n$ items that corresponds to \{$i_1, ..., i_n$\} s.t. $i_{o_k} = k$ }. $\{o_1, ...,o_n\}$ and \{$i_1, ..., i_n$\} have a one-to-one relationship
	
	%pseudo components 
	\item {Let $g(i_1, ..., i_n)$ be any probability density on $\mathcal{P}_n$. \\$Q(\tilde{\bm{\rho}}|\bm{R}^1, ..., \bm{R}^N, \alpha^0)$=$\sum\limits_{\{i_1, ..., i_n \}\in \mathcal{P}_n }q(\tilde{\bm{\rho}}|i_1, ..., i_n, \alpha^0, \bm{R}^1, ...,\bm{R}^N) \cdot g(i_1, ..., i_n)$ : pseudolikelihood, for $\tilde{\bm{\rho}} \in \mathcal{P}_n$, which approximates the Mallows posterior}
	
	\item {$q(\tilde{\bm{\rho}}|i_1, ..., i_n, \alpha^0, \bm{R}^1, ..., \bm{R}^N)  = q(\tilde{\bm{\rho}}|o_1, ..., o_n, \alpha^0, \bm{R}^1, ..., \bm{R}^N)  \\
		=q(\tilde{\rho}_{o_1}|\alpha^0,o_1, R^1_{o_1},...,R^N_{o_1}) \cdot
		q(\tilde{\rho}_{o_2}|\alpha^0,o_2,\tilde{\rho}_{o_1} R^1_{o_2},...,R^N_{o_2}) \cdot
		... \cdot \\
		q(\tilde{\rho}_{o_{n-1}}|\alpha^0,o_{n-1}, \tilde{\rho}_{o_1},...,\tilde{\rho}_{o_{n-2}}, R^1_{n-1},...,R^N_{n-1}) \cdot
		q(\tilde{\rho}_{o_{n}}|\alpha^0,o_{n}, \tilde{\rho}_{o_1},...,\tilde{\rho}_{o_{n-1}}, R^1_n, ..., R^N_n)$}

	where {	$ {q(\tilde{\rho}_{o_1}|\alpha^0, o_1,R^1_{o_1}, ...,R^N_{o_1}) }
	= \frac{\text{exp}\{- \frac{\alpha^0}{n}\sum\limits_{j=1}^{N}d(R^j_{o_1}, \tilde{\rho}_{o_1})\}}
	{\sum\limits_{\tilde{r}_{o_1}\in \{1, .., n\}}\text{exp}\{- \frac{\alpha^0}{n}\sum\limits_{j=1}^{N}d(R^j_{o_1}, \tilde{r}_{o_1})\}} $}, ${\tilde{\rho}_{o_1}\in \{1, ...,n\}}$, and where {
	$ {q(\tilde{\rho}_{o_k}|\alpha^0, o_k, \tilde{\rho}_{o_1}, ..., \tilde{\rho}_{o_{k-1}},R^1_{o_k}, ...,R^N_{o_k}) }
	= \frac{\text{exp}\{- \frac{\alpha^0}{n}\sum\limits_{j=1}^{N}d(R^j_{o_k}, \tilde{\rho}_{o_k})\}}
	{\sum\limits_{\tilde{r}_{o_k}\in \{1, .., n\}\textbackslash \{\tilde{\rho}_{o_1}, ..., \tilde{\rho}_{o_{k-1}}\}}\text{exp}\{- \frac{\alpha^0}{n}\sum\limits_{j=1}^{N}d(R^j_{o_k}, \tilde{r}_{o_k})\}} $}, ${\tilde{\rho}_{o_k}\in\{1, .., n\}\textbackslash \{\tilde{\rho}_{o_1}, ..., \tilde{\rho}_{o_{k-1}}\}}$,  for $k = 2, ..., n$



	\item{ $\bm{o}^0$: a set of ordering that corresponds to $\bm{\rho}^0$ s.t. ${{\rho}^0}^{-1}(m) = o^0_m$, i.e., $o^0_m$ is the item with rank $m$ in $\bm{\rho}^0$}
	
	
	\item{Define the ``v-function'' $f_v(\cdot)$ such that $f_v(\bm{\rho}^0)$ = $\mathcal{V}_{\bm{\rho}^0}$}, where 
	\begin{itemize}
			\item{ $\mathcal{V}_{\bm{\rho}^o}$ = $
			\left \{ 
			\begin{aligned}
			&\{\bm{r}\in \mathcal{P}_n: r_{o^0_m}=1, r_{o^0_{m\pm k}} \in \{2k, 2k+1 \}, k = 1, ..., m-1 \}, && \text{if n is odd} \\
			&\{\bm{r}\in \mathcal{P}_n: \{r_{o^0_{m-k}},r_{o^0_{m+k+1}} \} \in \{2k+1, 2k+2 \}, k = 0, ..., m \}, && \text{if n is even} 
			\end{aligned} \right.
			$
		}
	\end{itemize}


\end{itemize}

\section{Theorems and Lemmas}
\subsection{ }
\begin{lemma} \label{lem:nocross}
Let $\bm{R}\in \mathcal{P}_n$ be distributed according to $Mallows(\bm{\rho}^0, \alpha^0)$, for any fixed $\bm{\rho^0}$ and $\alpha^0$, if $n$ is odd, i.e., $n = 2m - 1$. $\forall \alpha^0 \in (0, \infty)$, the following is true.

	\begin{enumerate} 
		\item{$\mathbb{E}(R_{o^0_m}|\bm{\rho}_{0}, \alpha^0) = \rho^0_{o_m} = m$ }
		\item{$\forall j \in [1, m-2]$, $j< \mathbb{E}[R_{o^0_j}|\bm{\rho}^0, \alpha^0] <\mathbb{E}[R_{o^0_{j+1}}|\bm{\rho}^0, \alpha^0] < m$}
		\item{$\forall j \in [m+2, 2m-1]$, $m< \mathbb{E}[R_{o^0_{j-1}}|\bm{\rho}^0, \alpha^0] <\mathbb{E}[R_{o^0_{j}}|\bm{\rho}^0, \alpha^0] < j$}\\		
Similarly, if $n$ is even, i.e. $n = 2m$, 

	\item{$\forall j \in [1, m-1]$, $j< \mathbb{E}[R_{o^0_j}|\bm{\rho}^0, \alpha^0] <\mathbb{E}[R_{o^0_{j+1}}|\bm{\rho}^0, \alpha^0]$}
	\item{$\forall j \in [m+2, 2m]$, $\mathbb{E}[R_{o^0_{j-1}}|\bm{\rho}^0, \alpha^0] <\mathbb{E}[R_{o^0_{j}}|\bm{\rho}^0, \alpha^0] < j$}
\end{enumerate}
For both cases, $\forall 1\leq j<k\leq n$ and $\forall \alpha > 0 $,
{$\mathbb{E}[R_{o^0_j}|\bm{\rho}^0, \alpha^0]<\mathbb{E}[R_{o^0_k}|\bm{\rho}^0, \alpha^0]$}
\end{lemma}

\begin{lemma}\label{lem:largenumbers}
 Let $\bm{R}^j$ be distributed according to $Mallows (\bm{\rho}^0, \alpha^0)$, for $j = 1, ...,N$, independently, as $N \rightarrow \infty$, $\frac{1}{N}\sum\limits_{j=1}^{N}R^j_i \rightarrow \mathbb{E}[R_i|\bm{\rho}^0, \alpha ^0]$, $\forall i = 1, ..., n$
\end{lemma}

\begin{mydef}
The vector of ranks $\{r_1, ..., r_n \}  \in \mathcal{P}_n$ of a vector of real numbers $\{x_1, ..., x_n \}$, denoted as $rank(x_1, ..., x_n)$, is defined by $r_i(x_1, ..., x_n) = \sum\limits_{j=1}^{n}\delta (x_i - x_j)$, where 
$
\delta(x) = \left \{
\begin{aligned}
&1, && \text{if } x\geq 0 \\
&0, && \text{if } x < 0
\end{aligned} \right.
$, for $i = 1, ..., n$.
\end{mydef} 
\begin{theorem}\label{theorem:inferrho}
As $N \rightarrow \infty$, and $\forall \alpha > 0$, \\ $rank({1\over N}\sum\limits_{j=0}^{N}R_1^j,...,{1\over N}\sum\limits_{j=0}^{N}R_n^j) \rightarrow rank(\mathbb{E}[{R}_1|\bm{\rho}^0, \alpha_0],...,\mathbb{E}[{R}_n|\bm{\rho}^0, \alpha_0]) =\bm{\rho}^0 $
\end{theorem}

To rephrase, as $N$ approaches infinity, the Mallows consensus parameter $\bm{\rho}^0$ can be inferred from the data by taking the marginal mean for each item and then apply the rank function to these marginal means.
\subsection{ }
\begin{mydef}{
Let $\mathcal{V}_{\rho^0}$ be a set of permutations, determined by $\rho^0$, in the following way. If $n$ is odd:
 $\mathcal{V}_{\rho^0} = \{\bm{r}\in \mathcal{P}_n: r_{o^0_m} = 1; (r_{o^0_{m-k}},r_{o^0_{m+k}}) = (2k, 2k+1) $ or $ (2k+1, 2k)$, $k = 1, ..., m-1\}$. \\
 If $n$ is even,  $\mathcal{V}_{\rho^0} = \{\bm{r}\in \mathcal{P}_n:(r_{o^0_{m-k}},r_{o^0_{m+k+1}}) = (2k+1, 2k+2) $ or $ (2k+2, 2k+1)$, $k = 0, ..., m-1\}$.

}
\end{mydef}
\begin{theorem}\label{theorem:V}
Let	$\mathcal{D}_{\bm{\rho}^0}$ be the set of all distrbutions on $\mathcal{P}_n, \text{which can depend on } \bm{\rho}^0$. For a function $g$ defined on $\mathcal{P}_n$ which can depend on $\bm{\rho}^0$, for any given $n$ and $\alpha^0 > 0$, it holds that

 $\operatorname*{arg\,min}\limits_{g\in\mathcal{D}_{\bm{\rho}^0}}\lim\limits_{N \rightarrow \infty} KL (P(\bm{\rho}|\alpha^0, \bm{R}^1, ...,\bm{R}^N ) || \sum\limits_{\{i_1,..., i_n\} \in \mathcal{P}_n} q (\tilde{\bm{\rho}}|\alpha^0, \bm{R}^1,..., \bm{R}^N, i_1,...,i_n) g(i_1,...,i_n|\bm{\rho}^0)$ \\
= $g^*(i_1,...,i_n|\mathcal{V}_{{\bm{\rho}}^0})$, \\where
		$g^*(i_1, ..., i_n | \mathcal{V}_{\bm{\rho}^0})$ is a distribution whose density is concentrated on $\mathcal{V}_{\bm{\rho}^0}$, defined as \\
		  $
		\left \{
		\begin{aligned}
		&g^*(i_1, ..., i_n | \mathcal{V}_{\bm{\rho}^0}) = |\mathcal{V}_{\bm{\rho}^0}|^{-1}>0 , && \text{if } \{i_1,...,i_n\}\in \mathcal{V}_{\bm{\rho}^0}\\
		&g^*(i_1, ..., i_n | \mathcal{V}_{\bm{\rho}^0}) = 0 , && \text{if } \{i_1,...,i_n\}\notin \mathcal{V}_{\bm{\rho}^0}
		\end{aligned} \right.
		$, where $|\mathcal{V}_{\bm{\rho}^0}|
			= \left \{
			\begin{aligned}
			&2^{m-1}, && \text{if n is odd} \\
			&2^m, && \text{otherwise}
			\end{aligned} \right.
			$

\end{theorem}
As the number of users $N \rightarrow \infty$, the distribution $g^*$ that minimizes the KL-divergence between the Mallows posterior and the pseudolikelihood based on $g$, is the uniform distribution concentrated on $\mathcal{V}_{\bm{\rho}^o}$

\subsection{ }
For a given finite $N < \infty$, define $\hat{\bm{\rho}}^0$ = $rank({1\over N}\sum\limits_{j=0}^{N}R_1^j, ..., {1\over N}\sum\limits_{j=0}^{N}R_n^j)$ and  $\mathcal{V}_{\hat{\bm{\rho}}^0}$, 

\begin{theorem} \label{theorem:gaussiannoise}

$\exists\sigma \geq 0$ and $g'(i_1, ..., i_n|\mathcal{V}_{\hat{\bm{\rho}}^0}, \sigma)$ such that 

 KL $\Big(P(\bm{\rho}|\alpha^0, \bm{R}^1, ...,\bm{R}^N ) || \sum\limits_{\{i_1,..., i_n\} \in \mathcal{P}_n} q (\tilde{\bm{\rho}}|\alpha^0, \bm{R}^1,..., \bm{R}^N, i_1,...,i_n) g^{*}(i_1,...,i_n|\mathcal{V}_{\hat{\bm{\rho}}^0})\Big)\geq $\\
 KL $\Big(P(\bm{\rho}|\alpha^0, \bm{R}^1, ...,\bm{R}^N ) || \sum\limits_{\{i_1,..., i_n\} \in \mathcal{P}_n} q (\tilde{\bm{\rho}}|\alpha^0, \bm{R}^1,..., \bm{R}^N, i_1,...,i_n) g'(i_1,...,i_n|\mathcal{V}_{\hat{\bm{\rho}}^0}, \sigma)$\Big)\\

 where 
 $g'(i_1, ..., i_n|\mathcal{V}_{\hat{\bm{\rho}}^0}, \sigma) $ is defined as 
 \begin{itemize}
 	\item {$\hat{\bm{v}} \sim g^* (\hat{\bm{v}}|\mathcal{V}_{\hat{\bm{\rho}^{0}}})$}
  	\item {$x_i\sim \mathcal{N}(x_i|\hat{v}_i, \sigma)$} for $i = 1,...,n $
 	\item {$i_1, ..., i_n = rank(x_1, ..., x_n)$ }. 
 \end{itemize}
 	
\end{theorem} 
As $N$ is finite, $\bm{\rho}^0$ and therefore, $\mathcal{V}_{\bm{\rho}^0}$ usually cannot be accurately inferred from the data. We can however, sample $i_1, ..., i_n$ by sampling each item $i$ from a univariate Gaussian distribution centered on $\hat{v}_i$ with a fixed variance $\sigma$ for all items, and then obtain their ranking using the rank function. By introducing the variance, a smaller KL divergence of the pseudolikelihood from the Mallows posterior can be achieved.
 
\subsection{ }
\begin{theorem}\label{theorem:sigmafunction}
With the use of  $g'(i_1, ...,i_n|\mathcal{V}_{\hat{\bm{\rho}}^0}, \sigma)$, the value of $\sigma$ that minimizes the KL-divergence between the Mallows posterior and the resulting pseudolikelihood is\\
 $
\sigma = \left \{
\begin{aligned}
&0, && \text{if } \delta(\alpha^0,n,N)\leq \delta ^{*} \\
&f(\alpha^0, n,N), && \text{otherwise}
\end{aligned} \right.
$, where $\frac{\partial f}{\partial \alpha^0} <0, \frac{\partial f}{\partial N} <0$, and $\frac{\partial f}{\partial n} >0$, where $\delta(\alpha^0, n,N)$ is a specific function of $\alpha^0$, $n$, $N$ and $\delta ^*$ is a threshold.
\end{theorem}

In other words, $\sigma$ should be 0 when $\delta(\alpha^0,n,N) \leq \delta^*$. Beyond this point, the optimal choice of $\sigma$ should be greater than 0, and it follows a function $f(\alpha^0,n,N)$, which increases as $n$ increases, and decreases as $\alpha^0$ and $N$ increase.
\begin{corollary}
	As $N \rightarrow \infty,\sigma \rightarrow 0$  $ \forall \alpha >0$ and $n \geq 1$
\end{corollary}

\section{Evidence and proofs}
\subsection{Evidence for Theorem \ref{theorem:V}}
In this experiment, we first simulate a dataset of $N$ users and $n$ items with different choices of $\alpha^0$ by drawing from the Mallows distribution.  50 datasets are generated for each scenario. We have chosen $N = 2000$ so that it is sufficiently large and the characteristics as $N \rightarrow \infty$ can be simulated. As shown in \textbf{Figure} \ref{fig:boxPlots}, we have selected several different choices of $g$ functions for the pseudolikelihoods, and we compare their KL-divergences with the Mallows posterior. \textcolor{red}{Due to the difficulty of computing the KL-divergence between two distributions on the space of permutation $\mathcal{P}_n$, we instead compute the KL-divergence between the Mallows posterior and the pseudolikelihood's marginal distribution of each item, and we take the sum of all items' marginal KL-divergences. All the KL-divergences used in the following text are computed in this manner.} 

The different $g$ functions, following the sequence displayed in \textbf{Figure} \ref{fig:boxPlots}, are chosen as below:
\begin{itemize}
	\item {V: at each iteration, we sample one V-ranking \{$i_1, ...,i_n$\} from $g^*(i_1, ..., i_n | \mathcal{V}_{\bm{\rho}^0}))$, using a $\bm{\rho}^0$ estimated from the data}
	\item {opp\_V: at each iteration, we first sample one V-ranking \{$i_1, ...,i_n$\} from $g^*(i_1, ..., i_n | \mathcal{V}_{\bm{\rho}^0}))$, using a $\bm{\rho}^0$ estimated from the data, but then we generate the opposite V-ranking \{$n+1 - i_1, ..., n+1 - i_n$\}}
	\item {1\_n\_swap: at each iteration, we first fix the ranking to be \{1,2,...,n\}, and then conduct 5 swapping steps}
	\item {n\_1\_swap: we first fix the ordering to be \{n, n-1, ...,1\}, and then conduct 5 swapping steps at each iteration}
	\item {low\_std\_swap: we first compute each item's rank's standard deviation in the dataset and then rank them in ascending order according to this standard deviation. At each iteration, we conduct 5 swapping steps at each iteration based on this ranking }
	\item {high\_std\_swap: similar to low\_std\_swap, except that the standard deviations are ranked in descending order}
	\item{uniform: at each iteration, a ranking is drawn uniformly from $\mathcal{P}_n$}
\end{itemize}

It can be observed from \textbf{Figure} \ref{fig:boxPlots} that as $N$ increases, the ``V-rankings'' performs more and more superior compared to all the other $g$ functions. When $N = 2000$, asymptotic characteristics can be clearly observed, and as shown in the last row of \textbf{Figure} \ref{fig:boxPlots}, it consistently performs better compared to other choices. Expectedly, the opposite-V rankings' performance is the worst. However, it is noteworthy to mention that the asymptotic \textcolor{red}{? regime starts for larger $N$, for the situations when $n$ is large, and/or when $\alpha^0$ is small.} %in order to preserve the characteristics of $N \rightarrow \infty$, the required $N$ is larger 

\begin{figure}[h!]
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N500n10alpha1.pdf}
		
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N500n20alpha3.pdf}
		
	\end{minipage} 
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N500n30alpha7.pdf}
		
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N500n50alpha15.pdf}
		
	\end{minipage} 

	
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N1000n10alpha1.pdf}
		
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N1000n20alpha3.pdf}
		
	\end{minipage} 
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N1000n30alpha7.pdf}
		
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.23\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/theorem2_2/N1000n50alpha15.pdf}
		
	\end{minipage} 


	\begin{minipage}[t]{.23\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/N2000n10alpha1.pdf}
			
		\end{minipage}
		\hfill
		\begin{minipage}[t]{.23\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/N2000n20alpha3.pdf}
			
		\end{minipage} 
		\begin{minipage}[t]{.23\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/N2000n30alpha7.pdf}
			
		\end{minipage}
		\hfill
		\begin{minipage}[t]{.23\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/N2000n50alpha15.pdf}
			
		\end{minipage} 
	
	
	\caption{Comparison of KL-divergence when different choices of $g(i_1, ...,i_n|\bm{\rho}^0)$ is used. }
	\label{fig:boxPlots}
\end{figure}

Some selected heat plots which contains the Mallows posterior distribution and the pseuolikelihood with different choices of $g$ functions are shown in \textbf{Figure}\ref{fig:heatPlot_comparison_g}.  It can be clearly observed that the V-ranking is the optimal choice here. We also see here that the pseudolikelihod approximation based on $g^*$ also gives good results in terms of being very close to the true posterior.
\begin{figure}[h!]

		\begin{minipage}[t]{.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/heat_Mallows_N2000n10alpha1run1.pdf}
			
		\end{minipage}
		\hfill
		\begin{minipage}[t]{.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/heat_V_N2000n10alpha1run1.pdf}
			
		\end{minipage} 
		\hfill
		\begin{minipage}[t]{.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/heat_low_std_swap_N2000n10alpha1run1.pdf}
			
		\end{minipage} 
		\hfill
		\begin{minipage}[t]{.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/theorem2_2/heat_uniform_N2000n10alpha1run1.pdf}
			
		\end{minipage} 

	\caption{Comparison of heat plots when different $g$ functions are chosen. $N = 2000, n = 10, \alpha^0 = 1$, run 1}
	\label{fig:heatPlot_comparison_g}
\end{figure}


\subsection{Evidence for Theorem \ref{theorem:gaussiannoise}}
In \textbf{Figure} \ref{fig:sigmafunction}, for each subfigure, we calculate and plot the KL-divergences between the Mallows posterior and the pseudo-likelihood, computed with different choices of $\sigma$. The left-most point on each sub-figure corresponds to the KL-divergence when no Gausian variation is introduced, i.e. $\sigma = 0$. It can be observed that for most situations shown in the figure, the lowest KL-divergence is achieved when some level of Gaussian variation is introduced, especially as $N$ and $\alpha^0$ are relatively small. However, as $N$ and $\alpha^0$ increase, the optimal $\sigma$ appears to decrease towards 0. 

\textbf{Figure} \ref{fig:heatPlot_comparison} contains a few heat plot examples. The first column are heat plots of the Mallows posterior, the middle and right columns contain heat plots of the pseudolikelihood with and without Gaussian variations. It can be observed that introducing the Gaussian variation can reduce the difference between the Mallows posterior and the pseudolikelihood. This is largely due to the fact that since the true ``V-ranking'' cannot be accurately inferred from the data as $N$ is limited, we need to introduce some variation around the inferred ``V'' ranking in order to mitigate the inaccuracy.

\begin{figure}[h!]
		\subfigure[]{
		\begin{minipage}[t]{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Mallows_alpha1N100n10run1.pdf}
		
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Pseudo_alpha1N100n100run1sd0.pdf}

	\end{minipage} 
	\hfill
\begin{minipage}[t]{.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/Pseudo_alpha1N100n100run1sd2dot25.pdf}
	
\end{minipage} 

 }


	\subfigure[]{
		\begin{minipage}[t]{.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/Mallows_alpha1dot5N100n10run1.pdf}
			
		\end{minipage}
		\hfill
		\begin{minipage}[t]{.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/Pseudo_alpha1dot5N100n100run1sd0.pdf}
			
		\end{minipage} 
		\hfill
		\begin{minipage}[t]{.3\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/Pseudo_alpha1dot5N100n100run1sd1dot65.pdf}
			
		\end{minipage} 
		
	}

\subfigure[]{
	\begin{minipage}[t]{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Mallows_alpha2N100n10run1.pdf}
		
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Pseudo_alpha2N100n100run1sd0.pdf}
		
	\end{minipage} 
	\hfill
	\begin{minipage}[t]{.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/Pseudo_alpha2N100n100run1sd0dot75.pdf}
		
	\end{minipage} 
	
}

	
\caption{left column: Mallows; middle column: pseudolikelihood without Gaussian variation; right column: pseudolikelihood with Gaussian variation. $N = 200, n = 10$, $\alpha^0 = 1, 1.5, 2$ respectively, from top to bottom}
\label{fig:heatPlot_comparison}
\end{figure}
\subsection{Evidence for Theorem \ref{theorem:sigmafunction}}

\subsubsection{Optimal $\sigma$ is determined by $N, n, \alpha^0$}
As shown in \textbf{Figure} \ref{fig:sigmafunction}, in each subfigure, the $\sigma$ value that corresponds to the lowest KL-divergence is the optimal $\sigma$ for its specific ($N, n, \alpha^0$) set up. Each row of 3 figures shows a comparison of the optimal $\sigma$ when one of the variables ($N$, $n$, $\alpha$) changes. It can be observed that all three variables have an impact on the optimal value of $\sigma$. More specifically, the optimal choice of $\sigma$ appear to decrease as $N$ and $\alpha^0$ increase, and as $n$ decreases.
\begin{figure}[h!]
	\subfigure[]{
	\includegraphics[width=\linewidth]{figures/diff_N_users.pdf}\label{fig_diff_user}}
	\subfigure[]{
	\includegraphics[width=\linewidth]{figures/diff_n.pdf}\label{fig_diff_n}}
	\subfigure[]{
	\includegraphics[width=\linewidth]{figures/diff_alpha.pdf}\label{fig_diff_alpha}}
\caption{x-axis: $\sigma$, y-axis: KL-divergence}
\label{fig:sigmafunction}
\end{figure}

For each $N$, $n$ and $\alpha^0$, we simulate 10 datasets, and for each dataset, a grid of $\sigma$ values are tried out. In \textbf{Figure }\ref{fig:optimSigma}, we plot $\alpha^0$ on the x-axis, and its corresponded optimal $\sigma$ on the y-axis. It can be observed that up to a value $\delta ^*$, which increases as $N$ and $\alpha^0$ increase and decreases as $n$ increases, the lowest KL-divergence can be achieved by setting $\sigma$ to be 0. Beyond this point, the optimal $\sigma$ follows a function $f(\alpha^0, n,N)$, which increases as $N$ and $\alpha^0$ decreases and as $n$ increases.
\begin{figure}[h!]
		\subfigure[]{
		\includegraphics[width=\linewidth]{figures/2_4_sameN200diffn.pdf}\label{fig_optimSigma_diff_n}}
		\subfigure[]{
		\includegraphics[width=\linewidth]{figures/2_4_samen10diffN.pdf}\label{fig_optimSigma_diff_N}}
		\caption{x-axis: $\alpha^0$, y-axis: optimal $\sigma$}
		\label{fig:optimSigma}
\end{figure}

\subsubsection{Using data standard deviation / n as a proxy for $\alpha^0$}
Under most situations, the value of $\alpha^0$ is unknown, however, we can compute the marginal standard deviation of each item from the data, and normalize it by deviding it by n. The trend demonstrated in \textbf{Figure }\ref{fig:optimSigma} is well-preserved, as shown in \textbf{Figure }\ref{fig:optimSigma_sd_data}.

\begin{figure}[h!]\label{fig:optimSigma_sd_data}
	\subfigure[]{
		\includegraphics[width=\linewidth]{figures/2_4_sd_data_sameN200diffn.pdf}\label{fig_optimSigma_diff_n_sd_data}}
	\subfigure[]{
		\includegraphics[width=\linewidth]{figures/2_4_sd_data_samen10diffN.pdf}\label{fig_optimSigma_diff_N_sd_data}}
	\caption{x-axis: $\alpha^0$, y-axis: optimal $\sigma$}
\end{figure}

\end{document}  